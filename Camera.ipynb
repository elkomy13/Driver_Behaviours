{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Constants\n",
    "EYE_AR_THRESH_INITIAL = 0.76\n",
    "EYE_AR_CONSEC_FRAMES = 40\n",
    "HEAD_POSE_DOWN_THRESH = 89\n",
    "HEAD_POSE_RIGHT_THRESH = 70\n",
    "EAR_HISTORY_SIZE = 100\n",
    "\n",
    "# Initialize variables\n",
    "COUNTER = 0\n",
    "ALARM_ON = False\n",
    "ear_history = []\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)\n",
    "\n",
    "# Define indexes for the left and right eye from the MediaPipe landmarks\n",
    "LEFT_EYE_IDX = [33, 133, 160, 158, 144, 153]  # Left eye landmarks\n",
    "RIGHT_EYE_IDX = [362, 263, 387, 385, 373, 380]  # Right eye landmarks\n",
    "\n",
    "# Define indexes for head pose estimation (Nose and Eyes)\n",
    "NOSE_TIP_IDX = 1\n",
    "LEFT_EYE_INNER_IDX = 133\n",
    "RIGHT_EYE_INNER_IDX = 362\n",
    "\n",
    "# Head Pose Estimation Function\n",
    "def get_head_pose(landmarks, frame_shape):\n",
    "    nose_tip = np.array([landmarks.landmark[NOSE_TIP_IDX].x, landmarks.landmark[NOSE_TIP_IDX].y])\n",
    "    left_eye_inner = np.array([landmarks.landmark[LEFT_EYE_INNER_IDX].x, landmarks.landmark[LEFT_EYE_INNER_IDX].y])\n",
    "    right_eye_inner = np.array([landmarks.landmark[RIGHT_EYE_INNER_IDX].x, landmarks.landmark[RIGHT_EYE_INNER_IDX].y])\n",
    "\n",
    "    # Convert normalized coordinates to pixel coordinates\n",
    "    nose_tip *= np.array([frame_shape[1], frame_shape[0]])\n",
    "    left_eye_inner *= np.array([frame_shape[1], frame_shape[0]])\n",
    "    right_eye_inner *= np.array([frame_shape[1], frame_shape[0]])\n",
    "\n",
    "    # Compute head pose (simplified version)\n",
    "    nose_to_eyes_vector = nose_tip - (left_eye_inner + right_eye_inner) / 2\n",
    "    head_tilt_angle = np.degrees(np.arctan2(nose_to_eyes_vector[1], nose_to_eyes_vector[0]))\n",
    "\n",
    "    # Calculate bounding box\n",
    "    x_coords = [landmarks.landmark[idx].x * frame_shape[1] for idx in [33, 263, 1, 62]]\n",
    "    y_coords = [landmarks.landmark[idx].y * frame_shape[0] for idx in [33, 263, 1, 62]]\n",
    "    x_min, x_max = int(min(x_coords)), int(max(x_coords))\n",
    "    y_min, y_max = int(min(y_coords)), int(max(y_coords))\n",
    "\n",
    "    # Determine head pose direction\n",
    "# Determine head pose direction based on head tilt angle\n",
    "    if head_tilt_angle <= HEAD_POSE_DOWN_THRESH and head_tilt_angle > 80:\n",
    "        head_pose = \"Down\"\n",
    "    elif head_tilt_angle < HEAD_POSE_RIGHT_THRESH:\n",
    "        head_pose = \"Right\"\n",
    "    elif head_tilt_angle >= HEAD_POSE_RIGHT_THRESH and head_tilt_angle <= 80:\n",
    "        head_pose = \"Left\"\n",
    "    elif head_tilt_angle > HEAD_POSE_DOWN_THRESH:\n",
    "        head_pose = \"Forward\"\n",
    "    else:\n",
    "        head_pose = \"Unknown\"  # Fallback case if none of the conditions match\n",
    "\n",
    "    \n",
    "    return head_pose, head_tilt_angle, (x_min, y_min, x_max, y_max)\n",
    "\n",
    "# Define the eye aspect ratio function\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Video Capture\n",
    "\n",
    "# cap = cv2.VideoCapture(\"petal_20240814_134649.mp4\")\n",
    "# cap = cv2.VideoCapture(\"driver.mp4\")\n",
    "cap = cv2.VideoCapture(\"vid3.mp4\")\n",
    "\n",
    "\n",
    "width = 1280\n",
    "height = 720\n",
    "slow_down_factor = 1\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Resize the frame if needed\n",
    "        frame = cv2.resize(frame, (width, height))\n",
    "        \n",
    "        # Convert the frame to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect face landmarks using MediaPipe\n",
    "        results = face_mesh.process(rgb_frame)\n",
    "        \n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                # Estimate head pose and bounding box\n",
    "                head_pose, head_tilt_angle, (x_min, y_min, x_max, y_max) = get_head_pose(face_landmarks, frame.shape)\n",
    "                \n",
    "                # Draw bounding box around the head\n",
    "                cv2.rectangle(frame, (x_min - 50, y_min - 50), (x_max + 50, y_max + 60), (0, 255, 0), 2)\n",
    "                \n",
    "                # Check head pose and update drowsiness detection\n",
    "                if head_pose == \"Forward\":\n",
    "                    # Extract the left and right eye coordinates\n",
    "                    leftEye = np.array([(face_landmarks.landmark[idx].x, face_landmarks.landmark[idx].y) for idx in LEFT_EYE_IDX])\n",
    "                    rightEye = np.array([(face_landmarks.landmark[idx].x, face_landmarks.landmark[idx].y) for idx in RIGHT_EYE_IDX])\n",
    "                    \n",
    "                    # Convert normalized coordinates to pixel coordinates\n",
    "                    leftEye *= np.array([frame.shape[1], frame.shape[0]])\n",
    "                    rightEye *= np.array([frame.shape[1], frame.shape[0]])\n",
    "                    \n",
    "                    # Compute the eye aspect ratio for both eyes\n",
    "                    leftEAR = eye_aspect_ratio(leftEye)\n",
    "                    rightEAR = eye_aspect_ratio(rightEye)\n",
    "                    \n",
    "                    # Average the eye aspect ratio together for both eyes\n",
    "                    ear = (leftEAR + rightEAR) / 2.0\n",
    "\n",
    "                    # Update EAR history\n",
    "                    ear_history.append(ear)\n",
    "                    if len(ear_history) > EAR_HISTORY_SIZE:\n",
    "                        ear_history.pop(0)\n",
    "                    \n",
    "                    # Calculate dynamic threshold\n",
    "                    if len(ear_history) > 0:\n",
    "                        ear_mean = np.mean(ear_history)\n",
    "                        dynamic_threshold = max(EYE_AR_THRESH_INITIAL, ear_mean - 0.1)\n",
    "                    else:\n",
    "                        dynamic_threshold = EYE_AR_THRESH_INITIAL\n",
    "                    \n",
    "                    # Draw eye landmarks\n",
    "                    cv2.polylines(frame, [leftEye.astype(np.int32)], isClosed=True, color=(0, 255, 0), thickness=1)\n",
    "                    cv2.polylines(frame, [rightEye.astype(np.int32)], isClosed=True, color=(0, 255, 0), thickness=1)\n",
    "                    \n",
    "                    # Check if the eye aspect ratio is below the dynamic threshold\n",
    "                    if ear < dynamic_threshold:\n",
    "                        COUNTER += 1\n",
    "                        \n",
    "                        if COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
    "                            if not ALARM_ON:\n",
    "                                ALARM_ON = True\n",
    "                                print(\"Drowsiness Detected!\")\n",
    "                                \n",
    "                            cv2.putText(frame, \"DROWSINESS ALERT!\", (10, 30),\n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                    else:\n",
    "                        COUNTER = 0\n",
    "                        if ALARM_ON:\n",
    "                            ALARM_ON = False\n",
    "                        cv2.putText(frame, \"Good job, stay awake\", (10, 30),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                    \n",
    "                    # Display the computed eye aspect ratio\n",
    "                    cv2.putText(frame, \"EAR: {:.2f}\".format(ear), (300, 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                \n",
    "                # Display head tilt angle\n",
    "                cv2.putText(frame, f\"Head Tilt Angle: {head_tilt_angle:.2f}\", (10, 60),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                \n",
    "                # Display head pose\n",
    "                cv2.putText(frame, f\"Head Pose: {head_pose}\", (10, 90),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "                \n",
    "                # Check head pose and show alert for left/right\n",
    "                if head_pose == \"Down\":\n",
    "                    cv2.putText(frame, \"DROWSINESS ALERT! (Head Down)\", (10, 120),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                elif head_pose == \"Left\" or head_pose == \"Right\":\n",
    "                    cv2.putText(frame, \"Focus on the road\", (10, 120),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                \n",
    "        # Display the frame\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        \n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1000 // (30 // slow_down_factor)) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
